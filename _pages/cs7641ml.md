---
layout: archive
title: "Visual Question Answering using CLIP"
permalink: /cs7641ml/
author_profile: false
urlcolor: blue
---

### [Gunjan Aggarwal](https://gunagg.github.io/), [Harsh Maheshwari](https://harshm121.github.io/), [Ashwin Pathak](https://ashwinpathak20.github.io/), [Ram Ramrakhya](https://ram81.github.io/)

*lexicographical ordering



## Introduction

One of the tasks humans efficiently do is make sense of the visual information around us. However, we communicate our understanding of the visual information using natural language. Vision and Language usually go hand-in-hand for a lot of tasks we perform daily and thus it is important for machines to be able to do so as well. Visual Question Answering, introduced in [1], is one such task where the problem is to select one of many possible answers given an image and a question associated with the image. See figure 1 for an example. The VQA v2 [2] dataset has 82,783 training images which lead to 443,757 questions in total. Each question was answered by 10 humans generating a total of 4,437,570 answers.  There has been a lot of interest in multi-modal research, especially on vision and language modalities recently. CLIP [3] accelerated this research by releasing a large self-supervised trained model on a huge corpus of image, text pairs. Thanks to the easy access to the trained model, it has been used for various vision and language tasks such as zero-shot image classification [3], zero-shot text-to-image generation [4] and many others [5, 6]. We propose to leverage CLIP pre-trained embedding space to solve the VQA task.

![Figure 1: Examples of the image, question pairs from the VQA dataset](https://ashwinpathak20.github.io/images/vqaexample.png)

<div align="center"> Figure 1: Examples of the image, question pairs from the VQA dataset </div>



## Problem definition

Giving machines the ability to understand and communicate the understanding through question answering is an important and impactful task with a lot of social benefits like helping visually impaired humans [7]. However, it is often difficult and expensive to train huge models on large datasets. We propose to leverage the already available pre-trained vision-and-language CLIP model for the VQA task efficiently.

## Methods

**Supervised section**: CLIP has a visual encoder and a text encoder trained to align the vision and text embeddings when the inputs are aligned (see figure 2). We plan on adding a question-answer encoder that transforms the text embeddings from the question and the correct answer to align well with the image embedding (figure 3).

![Figure 2: CLIP training, we will use the pretrained CLIP model released by OpenAI](https://ashwinpathak20.github.io/images/clip.png)


<div align="center"> Figure 2: CLIP training, we will use the pretrained CLIP model released by OpenAI</div>



![Figure 3: Proposed method, where we freeze the CLIP encoders but only train the question answer encoder. Lock denotes frozen model and unlocked denotes trainable.](https://ashwinpathak20.github.io/images/proposed_method.png)

<div align="center"> Figure 3: Proposed method, where we freeze the CLIP encoders but only train the question answer encoder. Lock denotes frozen model and unlocked denotes trainable.</div>



**Unsupervised section**: While inference, the above approach would require encoding all possible answers to select the one which aligns the most. However, this is inefficient as the answers can be filtered out based on the question type. For example, the answer to “What color is …?” can never be “apple”. Thus if we are able to form clusters of answer types, we can use these clusters to filter out potential answers and reduce the computation cost while inference.

## Data Preprocessing
To enable zero-shot transfer we generate question-answer pairs using top 1000 answers from VQA train split, we use the top 1000 most frequent answers to generate question-answer pairs. For each question-answer pair we generate a corresponding text prompt by using fixed question  separator token ("Q:", "Question:", etc) and answer separator token ("A:", "Answer:", etc), for example "Question: What color is ....? Answer: apple".
Here's the preprocessing we perform for image input to CLIP:
1. Resize the input image to length of shortest edge
2. Center crop the image to 224x224
3. Normalize the image
To save training and evaluationg time for our model we precompute the image features for the static VQA dataset and save it on disk.
## Results and Discussion
### Unsupervised
To establish baselines to compare with our approach we start with experiments on zero-shot transfer of CLIP for VQA.
### Zero-Shot VQA using CLIP
In this approach we use pretrained CLIP text and image encoders to predict answer for the target question. To enable zero-shot transfer we generate question-answer pairs using top 1000 answers from VQA train split, we use the top 1000 most frequent answers to generate question-answer pairs. For each question-answer pair we generate a corresponding text prompt by using fixed question  separator token ("Q:", "Question:", etc) and answer separator token ("A:", "Answer:", etc), for example "Question: What color is ....? Answer: apple". Each question-answer prompt along with the image is passed through CLIP text and visual encoder to generate a similarity score, we then choose answer from the question-answer prompt with maximum similarity score as predicted answer. In these experiments, we focus on engineering the best prompt for zero-shot transfer of CLIP for VQA.
| Prompt                                       | other | number | yes/no |
| -------------------------------------------- | --- | --- | --- |
|< question > + " " + < answer > (Top 1000)                   |  7.01 |  0.06  |  0.07  |
|< question > + " " + < answer > (Top 3000)                   |  4.50 |  0.04  |  0.04  |
|Question: < question > + " " + Answer: < answer > (Top 1000) |  6.65 |  0.07  |  0.06  |
<p align="center">(1.) CLIP Zero-Shot transfer on VQA val. Accuracy for per question type</p>
Table 1. shows the results of zero-shot transfer of CLIP on VQA val split. First, we use a simple prompt that just appends <question> with a possible <answer> using space (" ") separator token. This prompt achieves 7.01% accuracy on questions of other category, 0.06% accuracy on questions which have number answer and 0.07% accuracy on yes/no questions (row 1). Next, we use the same prompt as row 1 but instead increase the set of candidate answers to top 3000 answers. We find that increasing the candidate answer search space leads to 2.51% worse accuracy on questions from other category, 0.03% worse accuracy on questions from yes/no category and 0.02% worse accuracy on questions from number category (row 1 vs 2). Finally, we use a prompt with separate question and answer separator. Following prompt uses "Question: " separator token followed by <question> and "Answer: " separator token followed by <answer> which are both appended using a space token (" "). This prompt performs 0.35% worse on questions from other category, 0.01% worse on questions from yes/no category and 0.01% better on questions from number category compared to a simple prompt (row 1 vs 3). Across our prompt engineering experiments we don't find significant improvements over our simple prompt baseline implying that prompt engineering is not enough to achieve good results on VQA.

### References

[1] S. Antol *et al.*, “Vqa: Visual question answering,” in *Proceedings of the IEEE international conference on computer vision*, 2015, pp. 2425–2433.

[2] Y Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

[3] A. Radford *et al.*, *Learning Transferable Visual Models From Natural Language Supervision*. 2021.

[4] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski, “StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery,” in *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, Oct. 2021, pp. 2085–2094.

[5] H. Xu, K. He, B. A. Plummer, L. Sigal, S. Sclaroff, and K. Saenko, “Multilevel language and vision integration for text-to-clip retrieval,” in *Proceedings of the AAAI Conference on Artificial Intelligence*, 2019, vol. 33, no. 1, pp. 9062–9069.

[6] M. Narasimhan, A. Rohrbach, and T. Darrell, “CLIP-It! language-guided video summarization,” *Advances in Neural Information Processing Systems  (NeurIPS), vol. 34, 2021.

[7] D. Gurari *et al.*, “Vizwiz grand challenge: Answering visual questions from blind people,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (CVPR), 2018, pp. 3608–3617.



## Proposed timeline and responsibilities

[Gantt Chart](https://docs.google.com/spreadsheets/d/1ocmk9Ot97a_fLugmLVr41tRd-i9ShB5s9uIF_GXADHM/edit#gid=1197748622)

![Gantt Chart](https://ashwinpathak20.github.io/images/ganttchart.png)

## Proposal video

[Video](https://www.youtube.com/watch?v=GTKwgFyt6N4)
