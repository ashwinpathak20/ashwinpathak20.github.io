---
title: 'ARDS Literature Survey'
date: 2022-08-22
permalink: /ards/2022/08/blog-post-1/
---

# Table of Contents
1. [Few-Shot Text Classification](#fewShotTextClassification)

# 1. Few-Shot Text Classification <a name="fewShotTextClassification"></a>

Article : https://few-shot-text-classification.fastforwardlabs.com/

This article uses a Zmap based approach to map the sentence level BERT based embedding to the Word2Vec based word embeddings for labels.

Such mapping provides a common ground for the sentences and labels to be able to compute the cosine distance correctly.
![Figure](https://few-shot-text-classification.fastforwardlabs.com/figures/cosinesim_wmap.gif)
Hence, such kind of technique is effective for "on-the-fly" or "zero-shot" training.

![Figure 1 : On the fly learning](https://few-shot-text-classification.fastforwardlabs.com/figures/lossfunction1.gif)
<div align="center">Figure 1 : On the fly learning</div>

Additionally, by adding a regularizer term to force to the weight matrix to be as close to identity matrix helps in few-shot learning as well.
![Figure 2 : Few-shot learning](https://few-shot-text-classification.fastforwardlabs.com/figures/lossfunction2.gif)
<div align="center">Figure 2 : Few-shot learning</div>

The datasets explored here are : AG News and Reddit

Few limitations
1. Validation can be challenging
2. Meaningful labels are a necessity
3. Supervised models are still better