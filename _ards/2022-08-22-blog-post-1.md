---
title: 'ARDS Literature Survey'
date: 2022-08-22
permalink: /ards/2022/08/blog-post-1/
---

# Table of Contents
1. [Few-Shot Text Classification](#fewShotTextClassification)
2. [Text classification from few training examples](#textclassificationfewExamples)

# 1. Few-Shot Text Classification <a name="fewShotTextClassification"></a>

Article : https://few-shot-text-classification.fastforwardlabs.com/

This article uses a Zmap based approach to map the sentence level BERT based embedding to the Word2Vec based word embeddings for labels.

Such mapping provides a common ground for the sentences and labels to be able to compute the cosine distance correctly.
![Figure](https://few-shot-text-classification.fastforwardlabs.com/figures/cosinesim_wmap.gif)
Hence, such kind of technique is effective for "on-the-fly" or "zero-shot" training.

![Figure 1 : On the fly learning](https://few-shot-text-classification.fastforwardlabs.com/figures/lossfunction1.gif)
<div align="center">Figure 1 : On the fly learning</div>

Additionally, by adding a regularizer term to force to the weight matrix to be as close to identity matrix helps in few-shot learning as well.
![Figure 2 : Few-shot learning](https://few-shot-text-classification.fastforwardlabs.com/figures/lossfunction2.gif)
<div align="center">Figure 2 : Few-shot learning</div>

The datasets explored here are : AG News and Reddit

Few limitations
1. Validation can be challenging
2. Meaningful labels are a necessity
3. Supervised models are still better

# 2. Text classification from few training examples <a name="textclassificationfewExamples"></a>

Article : https://maelfabien.github.io/machinelearning/NLP_5/#

This article proposes to use pre-trained models for classification in case of few-shot learning. It says there are several approaches to few shot learning in recent papers :

1. either use a Siamese Network based on LSTMs rather than CNNs, and use this for One-shot learning
2. learn word embeddings in one-shot or few-shot and classify on top
3. or use a pre-trained word / document embedding network, and build a metric on top

The article further explores method 3, using average of the embeddings to denote each class and finding closest distance during testing. 
Another method the article uses is to use KNNs after the embeddings to determine the label for the test class.

